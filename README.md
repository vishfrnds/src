
# Objective

Source code for language models, mostly based on transformers like llama.

Will be using pytorch and tinygrad for the framework. I feel currently pytorch is easier to work with but gets too complex for me beyond APIs. Tinygrad optimization looks simpler to understand and generic.
Ultimately might be CUDA on cpp for performance once best kernels are found.

# How to run

source build.sh